{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d3ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f550d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9b55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = train_X.astype(\"float64\")\n",
    "# train_y = train_y.astype(\"float64\")\n",
    "# test_X = test_X.astype(\"float64\")\n",
    "# test_y = test_X.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd8c3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_X/255\n",
    "train_X = train_X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73707b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "696fb549",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cf9efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_y(y):\n",
    "    output = []\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp = np.array([0]*10)\n",
    "        tmp[train_y[i]] = 1\n",
    "        output.append(tmp)\n",
    "    return np.array(output).T\n",
    "\n",
    "def get_flatten_X(X):\n",
    "    output = []\n",
    "    for i in range(X.shape[0]):\n",
    "        output.append(X[i].flatten())\n",
    "    return np.array(output).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeb0f182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_y_onehot = get_one_hot_y(train_y)\n",
    "train_X_flatten = get_flatten_X(train_X)\n",
    "test_y_onehot = get_one_hot_y(test_y)\n",
    "test_X_flatten = get_flatten_X(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2cf370d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 60000), (10, 10000))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_onehot.shape, test_y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e04f5e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 60000), (784, 10000))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_flatten.shape, test_X_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc527306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7\n",
      "Y onehot: [0 0 0 0 0 0 0 1 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_and_label_train_X(i):\n",
    "    print(\"Label:\", train_y[i])\n",
    "    print(\"Y onehot:\", train_y_onehot.T[i])\n",
    "    plt.gray()\n",
    "    plt.matshow(train_X[i])\n",
    "    plt.show()\n",
    "\n",
    "    # p = np.reshape(train_X_flatten.T[i].T,(28,28))\n",
    "    # plt.gray()\n",
    "    # plt.matshow(p)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plot_and_label_train_X(np.random.randint(0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "171c311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3653d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "#np.warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# init params\n",
    "W1 = np.random.uniform(-0.5, 0.5, (10,784))\n",
    "b1 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "W2 = np.random.uniform(-0.5, 0.5, (10,10))\n",
    "b2 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "\n",
    "i = 0\n",
    "m = 2\n",
    "\n",
    "X = train_X_flatten.T[i:i+m].T # shape -> 784, m\n",
    "Y = train_y_onehot.T[i:i+m].T\n",
    "\n",
    "# forward prop\n",
    "Z1 = W1@X+b1\n",
    "A1 = ReLU(Z1)\n",
    "Z2 = W2@A1+b2\n",
    "A2 = softmax(Z2)\n",
    "\n",
    "# backward prop\n",
    "dZ2 = A2-Y\n",
    "dW2 = 1/m*dZ2@A1.T\n",
    "db2 = 1/m*np.sum(dZ2)\n",
    "dZ1 = W2.T@dZ2 * ReLU_deriv(Z1)\n",
    "dW1 = 1/m*dZ1@X.T\n",
    "db1 = 1/m*np.sum(dZ1)\n",
    "\n",
    "# update poram\n",
    "learning_rate = 0.2\n",
    "W1 = W1 - learning_rate * dW1\n",
    "b1 = b1 - learning_rate * db1    \n",
    "W2 = W2 - learning_rate * dW2  \n",
    "b2 = b2 - learning_rate * db2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8438d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    # print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e09b3e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,) (10, 60000) (784, 60000)\n"
     ]
    }
   ],
   "source": [
    "print(train_y.shape ,train_y_onehot.shape, train_X_flatten.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60b3b24f",
   "metadata": {},
   "source": [
    "## TRAINING WITHOUT ITERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "090efcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "0.12431666666666667\n",
      "Iteration:  10\n",
      "0.17798333333333333\n",
      "Iteration:  20\n",
      "0.25956666666666667\n",
      "Iteration:  30\n",
      "0.36215\n",
      "Iteration:  40\n",
      "0.45436666666666664\n",
      "Iteration:  50\n",
      "0.5156\n",
      "Iteration:  60\n",
      "0.5605166666666667\n",
      "Iteration:  70\n",
      "0.5991\n",
      "Iteration:  80\n",
      "0.6289166666666667\n",
      "Iteration:  90\n",
      "0.65505\n",
      "Iteration:  100\n",
      "0.6771666666666667\n",
      "Iteration:  110\n",
      "0.6958\n",
      "Iteration:  120\n",
      "0.7109333333333333\n",
      "Iteration:  130\n",
      "0.7243166666666667\n",
      "Iteration:  140\n",
      "0.7363166666666666\n",
      "Iteration:  150\n",
      "0.7466\n",
      "Iteration:  160\n",
      "0.7560333333333333\n",
      "Iteration:  170\n",
      "0.7644\n",
      "Iteration:  180\n",
      "0.7716333333333333\n",
      "Iteration:  190\n",
      "0.7779666666666667\n",
      "Iteration:  200\n",
      "0.7834166666666667\n",
      "Iteration:  210\n",
      "0.7892666666666667\n",
      "Iteration:  220\n",
      "0.794\n",
      "Iteration:  230\n",
      "0.79835\n",
      "Iteration:  240\n",
      "0.8030833333333334\n",
      "Iteration:  250\n",
      "0.80675\n",
      "Iteration:  260\n",
      "0.8107666666666666\n",
      "Iteration:  270\n",
      "0.81445\n",
      "Iteration:  280\n",
      "0.8174833333333333\n",
      "Iteration:  290\n",
      "0.82065\n",
      "Iteration:  300\n",
      "0.8233166666666667\n",
      "Iteration:  310\n",
      "0.8259333333333333\n",
      "Iteration:  320\n",
      "0.8284833333333333\n",
      "Iteration:  330\n",
      "0.8307333333333333\n",
      "Iteration:  340\n",
      "0.83295\n",
      "Iteration:  350\n",
      "0.8349666666666666\n",
      "Iteration:  360\n",
      "0.8370666666666666\n",
      "Iteration:  370\n",
      "0.8389833333333333\n",
      "Iteration:  380\n",
      "0.8406166666666667\n",
      "Iteration:  390\n",
      "0.84235\n",
      "Iteration:  400\n",
      "0.8438166666666667\n",
      "Iteration:  410\n",
      "0.84535\n",
      "Iteration:  420\n",
      "0.8469166666666667\n",
      "Iteration:  430\n",
      "0.8484333333333334\n",
      "Iteration:  440\n",
      "0.8500833333333333\n",
      "Iteration:  450\n",
      "0.8513333333333334\n",
      "Iteration:  460\n",
      "0.8528166666666667\n",
      "Iteration:  470\n",
      "0.8541\n",
      "Iteration:  480\n",
      "0.8551166666666666\n",
      "Iteration:  490\n",
      "0.8563833333333334\n"
     ]
    }
   ],
   "source": [
    "W1 = np.random.uniform(-0.5, 0.5, (10,784))\n",
    "b1 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "W2 = np.random.uniform(-0.5, 0.5, (10,10))\n",
    "b2 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 500\n",
    "\n",
    "X = train_X_flatten # shape -> 784, m\n",
    "Y = train_y_onehot\n",
    "m = 60000\n",
    "\n",
    "correct_predictions = 0\n",
    "wrong_predictions = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # forward prop\n",
    "    Z1 = W1@X+b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2@A1+b2\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    # backward prop\n",
    "    dZ2 = A2-Y\n",
    "    dW2 = 1/m*dZ2@A1.T\n",
    "    db2 = 1/m*np.sum(dZ2)\n",
    "    dZ1 = W2.T@dZ2 * ReLU_deriv(Z1)\n",
    "    dW1 = 1/m*dZ1@X.T\n",
    "    db1 = 1/m*np.sum(dZ1)\n",
    "\n",
    "    # update poram\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1    \n",
    "    W2 = W2 - learning_rate * dW2  \n",
    "    b2 = b2 - learning_rate * db2    \n",
    "\n",
    "    # accuracy\n",
    "    if epoch % 10 == 0 :\n",
    "        # print(A2)\n",
    "        print(\"Epoch: \", epoch)\n",
    "        predictions = get_predictions(A2)\n",
    "        print(get_accuracy(predictions, train_y))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45804572",
   "metadata": {},
   "source": [
    "## IMPLEMENTING BATCH SIZE? BUT I DON'T KNOW HOW WOULD IT HELP? WHY NOT JUST USE ALL THE DATA AT ONCE\n",
    "\n",
    "Decreasing the batch size during training of a neural network can have several potential benefits:\n",
    "\n",
    "1- Reducing memory requirements: A smaller batch size means that fewer training examples are processed in each iteration, which can reduce the amount of memory required to store the training data and intermediate results. This can be important for training larger neural networks or when working with limited memory resources.\n",
    "\n",
    "2- More frequent weight updates: A smaller batch size means that the neural network is updated more frequently during each epoch, which can help to speed up the training process and potentially lead to better convergence. With a larger batch size, the weight updates are less frequent, and the optimizer may take longer to converge to the optimal weights.\n",
    "\n",
    "3- Improved generalization: Smaller batches can help to prevent the neural network from overfitting to the training data by introducing more randomness and variation into the training process. This can help the network to generalize better to new data and improve its performance on the validation and test sets.\n",
    "\n",
    "However, decreasing the batch size may also have some potential drawbacks:\n",
    "\n",
    "1- Slower training convergence: With smaller batch sizes, the optimization process may require more iterations or epochs to converge to the optimal weights, which can result in longer training times.\n",
    "\n",
    "2- Noisier weight updates: With smaller batch sizes, the gradient estimates may be noisier and less accurate, which can lead to more unstable training and slower convergence. This can be mitigated by using techniques such as momentum or weight decay.\n",
    "\n",
    "3- Overall, the optimal batch size will depend on the specific neural network architecture, dataset, and optimization algorithm used, and it may require experimentation and tuning to find the best value.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6147b2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_X_flatten.T.shape[0]//batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d04c9cfa",
   "metadata": {},
   "source": [
    "## TRAINING WITH ITERATIONS AND MANIPULATIVE BATCH SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8ff9cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Iteration: 0\n",
      "0.08321666666666666\n",
      "Epoch: 10  Iteration: 0\n",
      "0.16086666666666666\n",
      "Epoch: 20  Iteration: 0\n",
      "0.22696666666666668\n",
      "Epoch: 30  Iteration: 0\n",
      "0.30375\n",
      "Epoch: 40  Iteration: 0\n",
      "0.37248333333333333\n",
      "Epoch: 50  Iteration: 0\n",
      "0.42995\n",
      "Epoch: 60  Iteration: 0\n",
      "0.48643333333333333\n",
      "Epoch: 70  Iteration: 0\n",
      "0.5377666666666666\n",
      "Epoch: 80  Iteration: 0\n",
      "0.5785666666666667\n",
      "Epoch: 90  Iteration: 0\n",
      "0.611\n",
      "Epoch: 100  Iteration: 0\n",
      "0.63665\n",
      "Epoch: 110  Iteration: 0\n",
      "0.6580333333333334\n",
      "Epoch: 120  Iteration: 0\n",
      "0.6756333333333333\n",
      "Epoch: 130  Iteration: 0\n",
      "0.6904166666666667\n",
      "Epoch: 140  Iteration: 0\n",
      "0.7041166666666666\n",
      "Epoch: 150  Iteration: 0\n",
      "0.7163166666666667\n",
      "Epoch: 160  Iteration: 0\n",
      "0.72665\n",
      "Epoch: 170  Iteration: 0\n",
      "0.7356666666666667\n",
      "Epoch: 180  Iteration: 0\n",
      "0.74425\n",
      "Epoch: 190  Iteration: 0\n",
      "0.75155\n",
      "Epoch: 200  Iteration: 0\n",
      "0.7579666666666667\n",
      "Epoch: 210  Iteration: 0\n",
      "0.7637666666666667\n",
      "Epoch: 220  Iteration: 0\n",
      "0.7694\n",
      "Epoch: 230  Iteration: 0\n",
      "0.7739\n",
      "Epoch: 240  Iteration: 0\n",
      "0.7789833333333334\n",
      "Epoch: 250  Iteration: 0\n",
      "0.7837166666666666\n",
      "Epoch: 260  Iteration: 0\n",
      "0.7883333333333333\n",
      "Epoch: 270  Iteration: 0\n",
      "0.7917333333333333\n",
      "Epoch: 280  Iteration: 0\n",
      "0.7951833333333334\n",
      "Epoch: 290  Iteration: 0\n",
      "0.7980833333333334\n",
      "Epoch: 300  Iteration: 0\n",
      "0.8012333333333334\n",
      "Epoch: 310  Iteration: 0\n",
      "0.80405\n",
      "Epoch: 320  Iteration: 0\n",
      "0.80695\n",
      "Epoch: 330  Iteration: 0\n",
      "0.8096\n",
      "Epoch: 340  Iteration: 0\n",
      "0.8123166666666667\n",
      "Epoch: 350  Iteration: 0\n",
      "0.8144666666666667\n",
      "Epoch: 360  Iteration: 0\n",
      "0.8167833333333333\n",
      "Epoch: 370  Iteration: 0\n",
      "0.8191166666666667\n",
      "Epoch: 380  Iteration: 0\n",
      "0.8209833333333333\n",
      "Epoch: 390  Iteration: 0\n",
      "0.8229666666666666\n",
      "Epoch: 400  Iteration: 0\n",
      "0.8248833333333333\n",
      "Epoch: 410  Iteration: 0\n",
      "0.8268\n",
      "Epoch: 420  Iteration: 0\n",
      "0.8284833333333333\n",
      "Epoch: 430  Iteration: 0\n",
      "0.83025\n",
      "Epoch: 440  Iteration: 0\n",
      "0.8321\n",
      "Epoch: 450  Iteration: 0\n",
      "0.8335666666666667\n",
      "Epoch: 460  Iteration: 0\n",
      "0.8352833333333334\n",
      "Epoch: 470  Iteration: 0\n",
      "0.8369833333333333\n",
      "Epoch: 480  Iteration: 0\n",
      "0.8383833333333334\n",
      "Epoch: 490  Iteration: 0\n",
      "0.8397666666666667\n"
     ]
    }
   ],
   "source": [
    "W1 = np.random.uniform(-0.5, 0.5, (10,784))\n",
    "b1 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "W2 = np.random.uniform(-0.5, 0.5, (10,10))\n",
    "b2 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "\n",
    "\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 60000//1\n",
    "iterations = train_X_flatten.T.shape[0]//batch_size\n",
    "learning_rate = 0.1\n",
    "\n",
    "# m = batch_size\n",
    "X = train_X_flatten # shape -> 784, m\n",
    "Y = train_y_onehot # shape -< 10, m\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        X = train_X_flatten.T[(iteration*batch_size):(iteration+1)*batch_size].T\n",
    "        Y = train_y_onehot.T[(iteration*batch_size):(iteration+1)*batch_size].T\n",
    "        # print(iter*batch_size)\n",
    "        # forward prop\n",
    "        Z1 = W1@X+b1\n",
    "        A1 = ReLU(Z1)\n",
    "        Z2 = W2@A1+b2\n",
    "        A2 = softmax(Z2)\n",
    "\n",
    "        # backward prop\n",
    "        dZ2 = A2-Y\n",
    "        dW2 = 1/m*dZ2@A1.T\n",
    "        db2 = 1/m*np.sum(dZ2)\n",
    "        dZ1 = W2.T@dZ2 * ReLU_deriv(Z1)\n",
    "        dW1 = 1/m*dZ1@X.T\n",
    "        db1 = 1/m*np.sum(dZ1)\n",
    "\n",
    "        # update poram\n",
    "        W1 = W1 - learning_rate * dW1\n",
    "        b1 = b1 - learning_rate * db1    \n",
    "        W2 = W2 - learning_rate * dW2  \n",
    "        b2 = b2 - learning_rate * db2    \n",
    "\n",
    "        # accuracy\n",
    "        if epoch % 10 == 0 :\n",
    "            # print(A2)\n",
    "            print(\"Epoch:\", epoch, \" Iteration:\", iteration)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, train_y.T[(iteration*batch_size):(iteration+1)*batch_size].T))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "608fa870",
   "metadata": {},
   "source": [
    "## TEST ACCURACY WITH TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9453202e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8486"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = test_X_flatten\n",
    "Y = test_y\n",
    "\n",
    "Z1 = W1@X+b1\n",
    "A1 = ReLU(Z1)\n",
    "Z2 = W2@A1+b2\n",
    "A2 = softmax(Z2)\n",
    "\n",
    "get_accuracy(get_predictions(A2), Y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45139222",
   "metadata": {},
   "source": [
    "## TESTING WITH RANDOM INDIVIDUAL DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c9a771f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am % 49.77 certain that it is:  8\n",
      "Label: 6\n",
      "Y onehot: [0 0 0 0 0 0 1 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# index = 5151\n",
    "index = np.random.randint(0,1000)\n",
    "#plot_and_label_train_X(index)\n",
    "\n",
    "X = train_X_flatten.T[index:index+1].T\n",
    "y = train_y_onehot.T[index:index+1].T\n",
    "\n",
    "\n",
    "Z1 = W1@X+b1\n",
    "A1 = ReLU(Z1)\n",
    "Z2 = W2@A1+b2\n",
    "A2 = softmax(Z2)\n",
    "\n",
    "# print(Z1.shape, W1.shape, X.shape) # -> HOW????\n",
    "print(f\"I am % {np.around(np.max(A2)*100, 2)} certain that it is: \", np.argmax(A2))\n",
    "plot_and_label_train_X(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05665cec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W1 = np.random.uniform(-0.5, 0.5, (10,784))\n",
    "b1 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "W2 = np.random.uniform(-0.5, 0.5, (10,10))\n",
    "b2 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "\n",
    "learning_rate = 0.05\n",
    "iterations = 100\n",
    "\n",
    "\n",
    "\n",
    "m = 20000\n",
    "for i in range(0,60000, m):\n",
    "    X = train_X_flatten.T[i:i+m].T # shape -> 784, m\n",
    "    Y = train_y_onehot.T[i:i+m].T\n",
    "    print(i)\n",
    "    for iter in range(iterations):\n",
    "        # forward prop\n",
    "        Z1 = W1@X+b1\n",
    "        A1 = ReLU(Z1)\n",
    "        Z2 = W2@A1+b2\n",
    "        A2 = softmax(Z2)\n",
    "\n",
    "        # backward prop\n",
    "        dZ2 = A2-Y            \n",
    "        dW2 = 1/m*dZ2@A1.T\n",
    "        db2 = 1/m*np.sum(dZ2)\n",
    "        dZ1 = W2.T@dZ2 * ReLU_deriv(Z1)\n",
    "        dW1 = 1/m*dZ1@X.T\n",
    "        db1 = 1/m*np.sum(dZ1)\n",
    "\n",
    "        # update poram\n",
    "        W1 = W1 - learning_rate * dW1\n",
    "        b1 = b1 - learning_rate * db1    \n",
    "        W2 = W2 - learning_rate * dW2  \n",
    "        b2 = b2 - learning_rate * db2    \n",
    "\n",
    "       # accuracy\n",
    "        if iter % 10 == 0 :\n",
    "            # print(A2)\n",
    "            print(\"Iteration: \", iter)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, train_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd88bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(0,1000)\n",
    "plot_and_label_train_X(index)\n",
    "\n",
    "X = train_X_flatten.T[index:index+1].T # shape -> 784, m\n",
    "Y = train_y_onehot.T[index:index+1].T\n",
    "\n",
    "#print(X.shape)\n",
    "Z1 = W1@X+b1\n",
    "A1 = ReLU(Z1)\n",
    "Z2 = W2@A1+b2\n",
    "A2 = ReLU(Z2)\n",
    "print(A2.T[0])\n",
    "print(\"guess: \", np.argmax(A2.T[0]), \"| certainty: \" ,np.max(A2.T[0]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b8a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "spiral_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
