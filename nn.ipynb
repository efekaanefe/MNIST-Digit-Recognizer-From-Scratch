{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d3ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f550d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9b55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = train_X.astype(\"float64\")\n",
    "# train_y = train_y.astype(\"float64\")\n",
    "# test_X = test_X.astype(\"float64\")\n",
    "# test_y = test_X.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd8c3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_X/255\n",
    "train_X = train_X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73707b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "696fb549",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cf9efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_y(y):\n",
    "    output = []\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp = np.array([0]*10)\n",
    "        tmp[train_y[i]] = 1\n",
    "        output.append(tmp)\n",
    "    return np.array(output).T\n",
    "\n",
    "def get_flatten_X(X):\n",
    "    output = []\n",
    "    for i in range(X.shape[0]):\n",
    "        output.append(X[i].flatten())\n",
    "    return np.array(output).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeb0f182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_y_onehot = get_one_hot_y(train_y)\n",
    "train_X_flatten = get_flatten_X(train_X)\n",
    "test_y_onehot = get_one_hot_y(test_y)\n",
    "test_X_flatten = get_flatten_X(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2cf370d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 60000), (10, 10000))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_onehot.shape, test_y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e04f5e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 60000), (784, 10000))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_flatten.shape, test_X_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc527306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7\n",
      "Y onehot: [0 0 0 0 0 0 0 1 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_and_label_train_X(i):\n",
    "    print(\"Label:\", train_y[i])\n",
    "    print(\"Y onehot:\", train_y_onehot.T[i])\n",
    "    plt.gray()\n",
    "    plt.matshow(train_X[i])\n",
    "    plt.show()\n",
    "\n",
    "    # p = np.reshape(train_X_flatten.T[i].T,(28,28))\n",
    "    # plt.gray()\n",
    "    # plt.matshow(p)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plot_and_label_train_X(np.random.randint(0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "171c311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3653d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "#np.warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# init params\n",
    "W1 = np.random.uniform(-0.5, 0.5, (10,784))\n",
    "b1 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "W2 = np.random.uniform(-0.5, 0.5, (10,10))\n",
    "b2 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "\n",
    "i = 0\n",
    "m = 2\n",
    "\n",
    "X = train_X_flatten.T[i:i+m].T # shape -> 784, m\n",
    "Y = train_y_onehot.T[i:i+m].T\n",
    "\n",
    "# forward prop\n",
    "Z1 = W1@X+b1\n",
    "A1 = ReLU(Z1)\n",
    "Z2 = W2@A1+b2\n",
    "A2 = softmax(Z2)\n",
    "\n",
    "# backward prop\n",
    "dZ2 = A2-Y\n",
    "dW2 = 1/m*dZ2@A1.T\n",
    "db2 = 1/m*np.sum(dZ2)\n",
    "dZ1 = W2.T@dZ2 * ReLU_deriv(Z1)\n",
    "dW1 = 1/m*dZ1@X.T\n",
    "db1 = 1/m*np.sum(dZ1)\n",
    "\n",
    "# update poram\n",
    "learning_rate = 0.2\n",
    "W1 = W1 - learning_rate * dW1\n",
    "b1 = b1 - learning_rate * db1    \n",
    "W2 = W2 - learning_rate * dW2  \n",
    "b2 = b2 - learning_rate * db2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8438d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    # print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e09b3e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,) (10, 60000) (784, 60000)\n"
     ]
    }
   ],
   "source": [
    "print(train_y.shape ,train_y_onehot.shape, train_X_flatten.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60b3b24f",
   "metadata": {},
   "source": [
    "## TRAINING WITHOUT EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "090efcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "0.12431666666666667\n",
      "Iteration:  10\n",
      "0.17798333333333333\n",
      "Iteration:  20\n",
      "0.25956666666666667\n",
      "Iteration:  30\n",
      "0.36215\n",
      "Iteration:  40\n",
      "0.45436666666666664\n",
      "Iteration:  50\n",
      "0.5156\n",
      "Iteration:  60\n",
      "0.5605166666666667\n",
      "Iteration:  70\n",
      "0.5991\n",
      "Iteration:  80\n",
      "0.6289166666666667\n",
      "Iteration:  90\n",
      "0.65505\n",
      "Iteration:  100\n",
      "0.6771666666666667\n",
      "Iteration:  110\n",
      "0.6958\n",
      "Iteration:  120\n",
      "0.7109333333333333\n",
      "Iteration:  130\n",
      "0.7243166666666667\n",
      "Iteration:  140\n",
      "0.7363166666666666\n",
      "Iteration:  150\n",
      "0.7466\n",
      "Iteration:  160\n",
      "0.7560333333333333\n",
      "Iteration:  170\n",
      "0.7644\n",
      "Iteration:  180\n",
      "0.7716333333333333\n",
      "Iteration:  190\n",
      "0.7779666666666667\n",
      "Iteration:  200\n",
      "0.7834166666666667\n",
      "Iteration:  210\n",
      "0.7892666666666667\n",
      "Iteration:  220\n",
      "0.794\n",
      "Iteration:  230\n",
      "0.79835\n",
      "Iteration:  240\n",
      "0.8030833333333334\n",
      "Iteration:  250\n",
      "0.80675\n",
      "Iteration:  260\n",
      "0.8107666666666666\n",
      "Iteration:  270\n",
      "0.81445\n",
      "Iteration:  280\n",
      "0.8174833333333333\n",
      "Iteration:  290\n",
      "0.82065\n",
      "Iteration:  300\n",
      "0.8233166666666667\n",
      "Iteration:  310\n",
      "0.8259333333333333\n",
      "Iteration:  320\n",
      "0.8284833333333333\n",
      "Iteration:  330\n",
      "0.8307333333333333\n",
      "Iteration:  340\n",
      "0.83295\n",
      "Iteration:  350\n",
      "0.8349666666666666\n",
      "Iteration:  360\n",
      "0.8370666666666666\n",
      "Iteration:  370\n",
      "0.8389833333333333\n",
      "Iteration:  380\n",
      "0.8406166666666667\n",
      "Iteration:  390\n",
      "0.84235\n",
      "Iteration:  400\n",
      "0.8438166666666667\n",
      "Iteration:  410\n",
      "0.84535\n",
      "Iteration:  420\n",
      "0.8469166666666667\n",
      "Iteration:  430\n",
      "0.8484333333333334\n",
      "Iteration:  440\n",
      "0.8500833333333333\n",
      "Iteration:  450\n",
      "0.8513333333333334\n",
      "Iteration:  460\n",
      "0.8528166666666667\n",
      "Iteration:  470\n",
      "0.8541\n",
      "Iteration:  480\n",
      "0.8551166666666666\n",
      "Iteration:  490\n",
      "0.8563833333333334\n"
     ]
    }
   ],
   "source": [
    "W1 = np.random.uniform(-0.5, 0.5, (10,784))\n",
    "b1 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "W2 = np.random.uniform(-0.5, 0.5, (10,10))\n",
    "b2 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "iterations = 500\n",
    "\n",
    "X = train_X_flatten # shape -> 784, m\n",
    "Y = train_y_onehot\n",
    "m = 60000\n",
    "\n",
    "correct_predictions = 0\n",
    "wrong_predictions = 0\n",
    "\n",
    "for iter in range(iterations):\n",
    "    # forward prop\n",
    "    Z1 = W1@X+b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2@A1+b2\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    # backward prop\n",
    "    dZ2 = A2-Y\n",
    "    dW2 = 1/m*dZ2@A1.T\n",
    "    db2 = 1/m*np.sum(dZ2)\n",
    "    dZ1 = W2.T@dZ2 * ReLU_deriv(Z1)\n",
    "    dW1 = 1/m*dZ1@X.T\n",
    "    db1 = 1/m*np.sum(dZ1)\n",
    "\n",
    "    # update poram\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1    \n",
    "    W2 = W2 - learning_rate * dW2  \n",
    "    b2 = b2 - learning_rate * db2    \n",
    "\n",
    "    # accuracy\n",
    "    if iter % 10 == 0 :\n",
    "        # print(A2)\n",
    "        print(\"Iteration: \", iter)\n",
    "        predictions = get_predictions(A2)\n",
    "        print(get_accuracy(predictions, train_y))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45804572",
   "metadata": {},
   "source": [
    "## IMPLEMENTING EPOCH? BUT I DON'T KNOW HOW WOULD IT HELP? WHY NOT JUST USE ALL THE DATA AT ONCE\n",
    "\n",
    "Decreasing the batch size during training of a neural network can have several potential benefits:\n",
    "\n",
    "1- Reducing memory requirements: A smaller batch size means that fewer training examples are processed in each iteration, which can reduce the amount of memory required to store the training data and intermediate results. This can be important for training larger neural networks or when working with limited memory resources.\n",
    "\n",
    "2- More frequent weight updates: A smaller batch size means that the neural network is updated more frequently during each epoch, which can help to speed up the training process and potentially lead to better convergence. With a larger batch size, the weight updates are less frequent, and the optimizer may take longer to converge to the optimal weights.\n",
    "\n",
    "3- Improved generalization: Smaller batches can help to prevent the neural network from overfitting to the training data by introducing more randomness and variation into the training process. This can help the network to generalize better to new data and improve its performance on the validation and test sets.\n",
    "\n",
    "However, decreasing the batch size may also have some potential drawbacks:\n",
    "\n",
    "1- Slower training convergence: With smaller batch sizes, the optimization process may require more iterations or epochs to converge to the optimal weights, which can result in longer training times.\n",
    "\n",
    "2- Noisier weight updates: With smaller batch sizes, the gradient estimates may be noisier and less accurate, which can lead to more unstable training and slower convergence. This can be mitigated by using techniques such as momentum or weight decay.\n",
    "\n",
    "3- Overall, the optimal batch size will depend on the specific neural network architecture, dataset, and optimization algorithm used, and it may require experimentation and tuning to find the best value.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6147b2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_X_flatten.T.shape[0]//batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d04c9cfa",
   "metadata": {},
   "source": [
    "## TRAINING WITH EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8ff9cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Iteration: 0\n",
      "0.08321666666666666\n",
      "Epoch: 10  Iteration: 0\n",
      "0.16086666666666666\n",
      "Epoch: 20  Iteration: 0\n",
      "0.22696666666666668\n",
      "Epoch: 30  Iteration: 0\n",
      "0.30375\n",
      "Epoch: 40  Iteration: 0\n",
      "0.37248333333333333\n",
      "Epoch: 50  Iteration: 0\n",
      "0.42995\n",
      "Epoch: 60  Iteration: 0\n",
      "0.48643333333333333\n",
      "Epoch: 70  Iteration: 0\n",
      "0.5377666666666666\n",
      "Epoch: 80  Iteration: 0\n",
      "0.5785666666666667\n",
      "Epoch: 90  Iteration: 0\n",
      "0.611\n",
      "Epoch: 100  Iteration: 0\n",
      "0.63665\n",
      "Epoch: 110  Iteration: 0\n",
      "0.6580333333333334\n",
      "Epoch: 120  Iteration: 0\n",
      "0.6756333333333333\n",
      "Epoch: 130  Iteration: 0\n",
      "0.6904166666666667\n",
      "Epoch: 140  Iteration: 0\n",
      "0.7041166666666666\n",
      "Epoch: 150  Iteration: 0\n",
      "0.7163166666666667\n",
      "Epoch: 160  Iteration: 0\n",
      "0.72665\n",
      "Epoch: 170  Iteration: 0\n",
      "0.7356666666666667\n",
      "Epoch: 180  Iteration: 0\n",
      "0.74425\n",
      "Epoch: 190  Iteration: 0\n",
      "0.75155\n",
      "Epoch: 200  Iteration: 0\n",
      "0.7579666666666667\n",
      "Epoch: 210  Iteration: 0\n",
      "0.7637666666666667\n",
      "Epoch: 220  Iteration: 0\n",
      "0.7694\n",
      "Epoch: 230  Iteration: 0\n",
      "0.7739\n",
      "Epoch: 240  Iteration: 0\n",
      "0.7789833333333334\n",
      "Epoch: 250  Iteration: 0\n",
      "0.7837166666666666\n",
      "Epoch: 260  Iteration: 0\n",
      "0.7883333333333333\n",
      "Epoch: 270  Iteration: 0\n",
      "0.7917333333333333\n",
      "Epoch: 280  Iteration: 0\n",
      "0.7951833333333334\n",
      "Epoch: 290  Iteration: 0\n",
      "0.7980833333333334\n",
      "Epoch: 300  Iteration: 0\n",
      "0.8012333333333334\n",
      "Epoch: 310  Iteration: 0\n",
      "0.80405\n",
      "Epoch: 320  Iteration: 0\n",
      "0.80695\n",
      "Epoch: 330  Iteration: 0\n",
      "0.8096\n",
      "Epoch: 340  Iteration: 0\n",
      "0.8123166666666667\n",
      "Epoch: 350  Iteration: 0\n",
      "0.8144666666666667\n",
      "Epoch: 360  Iteration: 0\n",
      "0.8167833333333333\n",
      "Epoch: 370  Iteration: 0\n",
      "0.8191166666666667\n",
      "Epoch: 380  Iteration: 0\n",
      "0.8209833333333333\n",
      "Epoch: 390  Iteration: 0\n",
      "0.8229666666666666\n",
      "Epoch: 400  Iteration: 0\n",
      "0.8248833333333333\n",
      "Epoch: 410  Iteration: 0\n",
      "0.8268\n",
      "Epoch: 420  Iteration: 0\n",
      "0.8284833333333333\n",
      "Epoch: 430  Iteration: 0\n",
      "0.83025\n",
      "Epoch: 440  Iteration: 0\n",
      "0.8321\n",
      "Epoch: 450  Iteration: 0\n",
      "0.8335666666666667\n",
      "Epoch: 460  Iteration: 0\n",
      "0.8352833333333334\n",
      "Epoch: 470  Iteration: 0\n",
      "0.8369833333333333\n",
      "Epoch: 480  Iteration: 0\n",
      "0.8383833333333334\n",
      "Epoch: 490  Iteration: 0\n",
      "0.8397666666666667\n"
     ]
    }
   ],
   "source": [
    "W1 = np.random.uniform(-0.5, 0.5, (10,784))\n",
    "b1 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "W2 = np.random.uniform(-0.5, 0.5, (10,10))\n",
    "b2 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "\n",
    "\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 60000//1\n",
    "iterations = train_X_flatten.T.shape[0]//batch_size\n",
    "learning_rate = 0.1\n",
    "\n",
    "# m = batch_size\n",
    "X = train_X_flatten # shape -> 784, m\n",
    "Y = train_y_onehot # shape -< 10, m\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        X = train_X_flatten.T[(iteration*batch_size):(iteration+1)*batch_size].T\n",
    "        Y = train_y_onehot.T[(iteration*batch_size):(iteration+1)*batch_size].T\n",
    "        # print(iter*batch_size)\n",
    "        # forward prop\n",
    "        Z1 = W1@X+b1\n",
    "        A1 = ReLU(Z1)\n",
    "        Z2 = W2@A1+b2\n",
    "        A2 = softmax(Z2)\n",
    "\n",
    "        # backward prop\n",
    "        dZ2 = A2-Y\n",
    "        dW2 = 1/m*dZ2@A1.T\n",
    "        db2 = 1/m*np.sum(dZ2)\n",
    "        dZ1 = W2.T@dZ2 * ReLU_deriv(Z1)\n",
    "        dW1 = 1/m*dZ1@X.T\n",
    "        db1 = 1/m*np.sum(dZ1)\n",
    "\n",
    "        # update poram\n",
    "        W1 = W1 - learning_rate * dW1\n",
    "        b1 = b1 - learning_rate * db1    \n",
    "        W2 = W2 - learning_rate * dW2  \n",
    "        b2 = b2 - learning_rate * db2    \n",
    "\n",
    "        # accuracy\n",
    "        if epoch % 10 == 0 :\n",
    "            # print(A2)\n",
    "            print(\"Epoch:\", epoch, \" Iteration:\", iteration)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, train_y.T[(iteration*batch_size):(iteration+1)*batch_size].T))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "608fa870",
   "metadata": {},
   "source": [
    "## TEST ACCURACY WITH TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9453202e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8486"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = test_X_flatten\n",
    "Y = test_y\n",
    "\n",
    "Z1 = W1@X+b1\n",
    "A1 = ReLU(Z1)\n",
    "Z2 = W2@A1+b2\n",
    "A2 = softmax(Z2)\n",
    "\n",
    "get_accuracy(get_predictions(A2), Y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45139222",
   "metadata": {},
   "source": [
    "## TESTING WITH RANDOM INDIVIDUAL DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9a771f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like:  9\n",
      "Label: 9\n",
      "Y onehot: [0 0 0 0 0 0 0 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# index = 5151\n",
    "index = np.random.randint(0,1000)\n",
    "#plot_and_label_train_X(index)\n",
    "\n",
    "X = train_X_flatten.T[index:index+1].T\n",
    "y = train_y_onehot.T[index:index+1].T\n",
    "\n",
    "\n",
    "Z1 = W1@X+b1\n",
    "A1 = ReLU(Z1)\n",
    "Z2 = W2@A1+b2\n",
    "A2 = softmax(Z2)\n",
    "\n",
    "# print(Z1.shape, W1.shape, X.shape) # -> HOW????\n",
    "print(\"It looks like: \", np.argmax(A2))\n",
    "plot_and_label_train_X(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05665cec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W1 = np.random.uniform(-0.5, 0.5, (10,784))\n",
    "b1 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "W2 = np.random.uniform(-0.5, 0.5, (10,10))\n",
    "b2 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "\n",
    "learning_rate = 0.05\n",
    "iterations = 100\n",
    "\n",
    "\n",
    "\n",
    "m = 20000\n",
    "for i in range(0,60000, m):\n",
    "    X = train_X_flatten.T[i:i+m].T # shape -> 784, m\n",
    "    Y = train_y_onehot.T[i:i+m].T\n",
    "    print(i)\n",
    "    for iter in range(iterations):\n",
    "        # forward prop\n",
    "        Z1 = W1@X+b1\n",
    "        A1 = ReLU(Z1)\n",
    "        Z2 = W2@A1+b2\n",
    "        A2 = softmax(Z2)\n",
    "\n",
    "        # backward prop\n",
    "        dZ2 = A2-Y            \n",
    "        dW2 = 1/m*dZ2@A1.T\n",
    "        db2 = 1/m*np.sum(dZ2)\n",
    "        dZ1 = W2.T@dZ2 * ReLU_deriv(Z1)\n",
    "        dW1 = 1/m*dZ1@X.T\n",
    "        db1 = 1/m*np.sum(dZ1)\n",
    "\n",
    "        # update poram\n",
    "        W1 = W1 - learning_rate * dW1\n",
    "        b1 = b1 - learning_rate * db1    \n",
    "        W2 = W2 - learning_rate * dW2  \n",
    "        b2 = b2 - learning_rate * db2    \n",
    "\n",
    "       # accuracy\n",
    "        if iter % 10 == 0 :\n",
    "            # print(A2)\n",
    "            print(\"Iteration: \", iter)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, train_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd88bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(0,1000)\n",
    "plot_and_label_train_X(index)\n",
    "\n",
    "X = train_X_flatten.T[index:index+1].T # shape -> 784, m\n",
    "Y = train_y_onehot.T[index:index+1].T\n",
    "\n",
    "#print(X.shape)\n",
    "Z1 = W1@X+b1\n",
    "A1 = ReLU(Z1)\n",
    "Z2 = W2@A1+b2\n",
    "A2 = ReLU(Z2)\n",
    "print(A2.T[0])\n",
    "print(\"guess: \", np.argmax(A2.T[0]), \"| certainty: \" ,np.max(A2.T[0]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b8a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "spiral_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
