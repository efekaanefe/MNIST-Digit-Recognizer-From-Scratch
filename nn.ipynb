{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d3ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f550d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a9b55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = train_X.astype(\"float64\")\n",
    "# train_y = train_y.astype(\"float64\")\n",
    "# test_X = test_X.astype(\"float64\")\n",
    "# test_y = test_X.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd8c3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_X/255\n",
    "train_X = train_X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73707b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "696fb549",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf9efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_y(y):\n",
    "    output = []\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp = np.array([0]*10)\n",
    "        tmp[train_y[i]] = 1\n",
    "        output.append(tmp)\n",
    "    return np.array(output).T\n",
    "\n",
    "def get_flatten_X(X):\n",
    "    output = []\n",
    "    for i in range(X.shape[0]):\n",
    "        output.append(X[i].flatten())\n",
    "    return np.array(output).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb0f182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_y_onehot = get_one_hot_y(train_y)\n",
    "train_X_flatten = get_flatten_X(train_X)\n",
    "test_y_onehot = get_one_hot_y(test_y)\n",
    "test_X_flatten = get_flatten_X(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2cf370d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 60000), (10, 10000))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_onehot.shape, test_y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e04f5e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 60000), (784, 10000))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_flatten.shape, test_X_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc527306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7\n",
      "Y onehot: [0 0 0 0 0 0 0 1 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_and_label_train_X(i):\n",
    "    print(\"Label:\", train_y[i])\n",
    "    print(\"Y onehot:\", train_y_onehot.T[i])\n",
    "    plt.gray()\n",
    "    plt.matshow(train_X[i])\n",
    "    plt.show()\n",
    "\n",
    "    # p = np.reshape(train_X_flatten.T[i].T,(28,28))\n",
    "    # plt.gray()\n",
    "    # plt.matshow(p)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plot_and_label_train_X(np.random.randint(0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "171c311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3653d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "#np.warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# init params\n",
    "W1 = np.random.uniform(-0.5, 0.5, (10,784))\n",
    "b1 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "W2 = np.random.uniform(-0.5, 0.5, (10,10))\n",
    "b2 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "\n",
    "i = 0\n",
    "m = 2\n",
    "\n",
    "X = train_X_flatten.T[i:i+m].T # shape -> 784, m\n",
    "Y = train_y_onehot.T[i:i+m].T\n",
    "\n",
    "# forward prop\n",
    "Z1 = W1@X+b1\n",
    "A1 = ReLU(Z1)\n",
    "Z2 = W2@A1+b2\n",
    "A2 = softmax(Z2)\n",
    "\n",
    "# backward prop\n",
    "dZ2 = A2-Y\n",
    "dW2 = 1/m*dZ2@A1.T\n",
    "db2 = 1/m*np.sum(dZ2)\n",
    "dZ1 = W2.T@dZ2 * ReLU_deriv(Z1)\n",
    "dW1 = 1/m*dZ1@X.T\n",
    "db1 = 1/m*np.sum(dZ1)\n",
    "\n",
    "# update poram\n",
    "learning_rate = 0.2\n",
    "W1 = W1 - learning_rate * dW1\n",
    "b1 = b1 - learning_rate * db1    \n",
    "W2 = W2 - learning_rate * dW2  \n",
    "b2 = b2 - learning_rate * db2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8438d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    # print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e09b3e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,) (10, 60000) (784, 60000)\n"
     ]
    }
   ],
   "source": [
    "print(train_y.shape ,train_y_onehot.shape, train_X_flatten.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60b3b24f",
   "metadata": {},
   "source": [
    "## TRAINING WITHOUT ITERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "090efcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "0.0916\n",
      "Epoch:  10\n",
      "0.11695\n",
      "Epoch:  20\n",
      "0.17801666666666666\n",
      "Epoch:  30\n",
      "0.22885\n",
      "Epoch:  40\n",
      "0.2622833333333333\n",
      "Epoch:  50\n",
      "0.3123166666666667\n",
      "Epoch:  60\n",
      "0.3534833333333333\n",
      "Epoch:  70\n",
      "0.38245\n",
      "Epoch:  80\n",
      "0.4081666666666667\n",
      "Epoch:  90\n",
      "0.4406\n",
      "Epoch:  100\n",
      "0.47533333333333333\n",
      "Epoch:  110\n",
      "0.50885\n",
      "Epoch:  120\n",
      "0.5382\n",
      "Epoch:  130\n",
      "0.5628166666666666\n",
      "Epoch:  140\n",
      "0.5844166666666667\n",
      "Epoch:  150\n",
      "0.6046666666666667\n",
      "Epoch:  160\n",
      "0.6228\n",
      "Epoch:  170\n",
      "0.6385833333333333\n",
      "Epoch:  180\n",
      "0.65485\n",
      "Epoch:  190\n",
      "0.6684333333333333\n",
      "Epoch:  200\n",
      "0.6809333333333333\n",
      "Epoch:  210\n",
      "0.6928833333333333\n",
      "Epoch:  220\n",
      "0.7042166666666667\n",
      "Epoch:  230\n",
      "0.7135166666666667\n",
      "Epoch:  240\n",
      "0.7221\n",
      "Epoch:  250\n",
      "0.7298\n",
      "Epoch:  260\n",
      "0.7373833333333333\n",
      "Epoch:  270\n",
      "0.74415\n",
      "Epoch:  280\n",
      "0.751\n",
      "Epoch:  290\n",
      "0.75705\n",
      "Epoch:  300\n",
      "0.7631833333333333\n",
      "Epoch:  310\n",
      "0.7681833333333333\n",
      "Epoch:  320\n",
      "0.7732333333333333\n",
      "Epoch:  330\n",
      "0.7778\n",
      "Epoch:  340\n",
      "0.7823333333333333\n",
      "Epoch:  350\n",
      "0.7866\n",
      "Epoch:  360\n",
      "0.7900833333333334\n",
      "Epoch:  370\n",
      "0.7936833333333333\n",
      "Epoch:  380\n",
      "0.79685\n",
      "Epoch:  390\n",
      "0.8001166666666667\n",
      "Epoch:  400\n",
      "0.8032833333333333\n",
      "Epoch:  410\n",
      "0.8066833333333333\n",
      "Epoch:  420\n",
      "0.8098333333333333\n",
      "Epoch:  430\n",
      "0.8122833333333334\n",
      "Epoch:  440\n",
      "0.8144333333333333\n",
      "Epoch:  450\n",
      "0.8168\n",
      "Epoch:  460\n",
      "0.8189666666666666\n",
      "Epoch:  470\n",
      "0.8213333333333334\n",
      "Epoch:  480\n",
      "0.82365\n",
      "Epoch:  490\n",
      "0.82565\n"
     ]
    }
   ],
   "source": [
    "W1 = np.random.uniform(-0.5, 0.5, (10,784))\n",
    "b1 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "W2 = np.random.uniform(-0.5, 0.5, (10,10))\n",
    "b2 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 500\n",
    "\n",
    "X = train_X_flatten # shape -> 784, m\n",
    "Y = train_y_onehot\n",
    "m = 60000\n",
    "\n",
    "correct_predictions = 0\n",
    "wrong_predictions = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # forward prop\n",
    "    Z1 = W1@X+b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2@A1+b2\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    # backward prop\n",
    "    dZ2 = A2-Y\n",
    "    dW2 = 1/m*dZ2@A1.T\n",
    "    db2 = 1/m*np.sum(dZ2)\n",
    "    dZ1 = W2.T@dZ2 * ReLU_deriv(Z1)\n",
    "    dW1 = 1/m*dZ1@X.T\n",
    "    db1 = 1/m*np.sum(dZ1)\n",
    "\n",
    "    # update poram\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1    \n",
    "    W2 = W2 - learning_rate * dW2  \n",
    "    b2 = b2 - learning_rate * db2    \n",
    "\n",
    "    # accuracy\n",
    "    if epoch % 10 == 0 :\n",
    "        # print(A2)\n",
    "        print(\"Epoch: \", epoch)\n",
    "        predictions = get_predictions(A2)\n",
    "        print(get_accuracy(predictions, train_y))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45804572",
   "metadata": {},
   "source": [
    "## IMPLEMENTING BATCH SIZE? BUT I DON'T KNOW HOW WOULD IT HELP? WHY NOT JUST USE ALL THE DATA AT ONCE\n",
    "\n",
    "Decreasing the batch size during training of a neural network can have several potential benefits:\n",
    "\n",
    "1- Reducing memory requirements: A smaller batch size means that fewer training examples are processed in each iteration, which can reduce the amount of memory required to store the training data and intermediate results. This can be important for training larger neural networks or when working with limited memory resources.\n",
    "\n",
    "2- More frequent weight updates: A smaller batch size means that the neural network is updated more frequently during each epoch, which can help to speed up the training process and potentially lead to better convergence. With a larger batch size, the weight updates are less frequent, and the optimizer may take longer to converge to the optimal weights.\n",
    "\n",
    "3- Improved generalization: Smaller batches can help to prevent the neural network from overfitting to the training data by introducing more randomness and variation into the training process. This can help the network to generalize better to new data and improve its performance on the validation and test sets.\n",
    "\n",
    "However, decreasing the batch size may also have some potential drawbacks:\n",
    "\n",
    "1- Slower training convergence: With smaller batch sizes, the optimization process may require more iterations or epochs to converge to the optimal weights, which can result in longer training times.\n",
    "\n",
    "2- Noisier weight updates: With smaller batch sizes, the gradient estimates may be noisier and less accurate, which can lead to more unstable training and slower convergence. This can be mitigated by using techniques such as momentum or weight decay.\n",
    "\n",
    "3- Overall, the optimal batch size will depend on the specific neural network architecture, dataset, and optimization algorithm used, and it may require experimentation and tuning to find the best value.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6147b2f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-98f6e31f4b73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_X_flatten\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "train_X_flatten.T.shape[0]//batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d04c9cfa",
   "metadata": {},
   "source": [
    "## TRAINING WITH ITERATIONS AND MANIPULATIVE BATCH SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff9cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.uniform(-0.5, 0.5, (10,784))\n",
    "b1 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "W2 = np.random.uniform(-0.5, 0.5, (10,10))\n",
    "b2 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "\n",
    "epochs = 250\n",
    "batch_size = 60000//1\n",
    "iterations = train_X_flatten.T.shape[0]//batch_size\n",
    "learning_rate = 0.65\n",
    "\n",
    "# m = batch_size\n",
    "X = train_X_flatten # shape -> 784, m\n",
    "Y = train_y_onehot # shape -< 10, m\n",
    "\n",
    "# plot accuracy\n",
    "accuracy_values = []\n",
    "epoch_values = []\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        X = train_X_flatten.T[(iteration*batch_size):(iteration+1)*batch_size].T\n",
    "        Y = train_y_onehot.T[(iteration*batch_size):(iteration+1)*batch_size].T\n",
    "        # print(iter*batch_size)\n",
    "        # forward prop\n",
    "        Z1 = W1@X+b1\n",
    "        A1 = ReLU(Z1)\n",
    "        Z2 = W2@A1+b2\n",
    "        A2 = softmax(Z2)\n",
    "\n",
    "        # backward prop\n",
    "        dZ2 = A2-Y\n",
    "        dW2 = 1/m*dZ2@A1.T\n",
    "        db2 = 1/m*np.sum(dZ2)\n",
    "        dZ1 = W2.T@dZ2 * ReLU_deriv(Z1)\n",
    "        dW1 = 1/m*dZ1@X.T\n",
    "        db1 = 1/m*np.sum(dZ1)\n",
    "\n",
    "        # update poram\n",
    "        W1 = W1 - learning_rate * dW1\n",
    "        b1 = b1 - learning_rate * db1    \n",
    "        W2 = W2 - learning_rate * dW2  \n",
    "        b2 = b2 - learning_rate * db2    \n",
    "\n",
    "        # accuracy\n",
    "        if epoch % 10 == 0:\n",
    "            # print(A2)\n",
    "            print(\"Epoch:\", epoch,)\n",
    "            predictions = get_predictions(A2)\n",
    "            accuracy = get_accuracy(predictions, train_y.T[(iteration*batch_size):(iteration+1)*batch_size].T)\n",
    "            print(accuracy)\n",
    "\n",
    "            accuracy_values.append(accuracy)\n",
    "            epoch_values.append(epoch)\n",
    "\n",
    "fig = plt.figure(1)\t#identifies the figure \n",
    "plt.title(f\"accuracy vs epoch = {epoch}, batch_size = {batch_size}, learning_rate = {learning_rate}, iterations = {iterations}\", fontsize='16')\t#title\n",
    "plt.plot(epoch_values, accuracy_values)\t#plot the points\n",
    "plt.xlabel(\"epoch\",fontsize='13')\t#adds a label in the x axis\n",
    "plt.ylabel(\"accuracy\",fontsize='13')\t#adds a label in the y axis\n",
    "# plt.savefig(f\"epoch_{epoch} batch_size_{batch_size}.png\")\t#saves the figure in the present directory\n",
    "\n",
    "plt.grid()\t#shows a grid under the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "608fa870",
   "metadata": {},
   "source": [
    "## TEST ACCURACY WITH TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9453202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = test_X_flatten\n",
    "Y = test_y\n",
    "\n",
    "Z1 = W1@X+b1\n",
    "A1 = ReLU(Z1)\n",
    "Z2 = W2@A1+b2\n",
    "A2 = softmax(Z2)\n",
    "\n",
    "get_accuracy(get_predictions(A2), Y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45139222",
   "metadata": {},
   "source": [
    "## TESTING WITH RANDOM INDIVIDUAL DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a771f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 5151\n",
    "index = np.random.randint(0,1000)\n",
    "#plot_and_label_train_X(index)\n",
    "\n",
    "X = train_X_flatten.T[index:index+1].T\n",
    "y = train_y_onehot.T[index:index+1].T\n",
    "\n",
    "\n",
    "Z1 = W1@X+b1\n",
    "A1 = ReLU(Z1)\n",
    "Z2 = W2@A1+b2\n",
    "A2 = softmax(Z2)\n",
    "\n",
    "print(f\"I am % {np.around(np.max(A2)*100, 2)} certain that it is: \", np.argmax(A2))\n",
    "plot_and_label_train_X(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05665cec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W1 = np.random.uniform(-0.5, 0.5, (10,784))\n",
    "b1 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "W2 = np.random.uniform(-0.5, 0.5, (10,10))\n",
    "b2 = np.random.uniform(-0.5, 0.5, (10,1))\n",
    "\n",
    "learning_rate = 0.05\n",
    "iterations = 100\n",
    "\n",
    "\n",
    "\n",
    "m = 20000\n",
    "for i in range(0,60000, m):\n",
    "    X = train_X_flatten.T[i:i+m].T # shape -> 784, m\n",
    "    Y = train_y_onehot.T[i:i+m].T\n",
    "    print(i)\n",
    "    for iter in range(iterations):\n",
    "        # forward prop\n",
    "        Z1 = W1@X+b1\n",
    "        A1 = ReLU(Z1)\n",
    "        Z2 = W2@A1+b2\n",
    "        A2 = softmax(Z2)\n",
    "\n",
    "        # backward prop\n",
    "        dZ2 = A2-Y            \n",
    "        dW2 = 1/m*dZ2@A1.T\n",
    "        db2 = 1/m*np.sum(dZ2)\n",
    "        dZ1 = W2.T@dZ2 * ReLU_deriv(Z1)\n",
    "        dW1 = 1/m*dZ1@X.T\n",
    "        db1 = 1/m*np.sum(dZ1)\n",
    "\n",
    "        # update poram\n",
    "        W1 = W1 - learning_rate * dW1\n",
    "        b1 = b1 - learning_rate * db1    \n",
    "        W2 = W2 - learning_rate * dW2  \n",
    "        b2 = b2 - learning_rate * db2    \n",
    "\n",
    "       # accuracy\n",
    "        if iter % 10 == 0 :\n",
    "            # print(A2)\n",
    "            print(\"Iteration: \", iter)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, train_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd88bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(0,1000)\n",
    "plot_and_label_train_X(index)\n",
    "\n",
    "X = train_X_flatten.T[index:index+1].T # shape -> 784, m\n",
    "Y = train_y_onehot.T[index:index+1].T\n",
    "\n",
    "#print(X.shape)\n",
    "Z1 = W1@X+b1\n",
    "A1 = ReLU(Z1)\n",
    "Z2 = W2@A1+b2\n",
    "A2 = ReLU(Z2)\n",
    "print(A2.T[0])\n",
    "print(\"guess: \", np.argmax(A2.T[0]), \"| certainty: \" ,np.max(A2.T[0]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b8a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "spiral_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
